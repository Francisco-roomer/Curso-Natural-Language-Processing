{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmo de Tokenizaci칩n Avanzada: BPE (Byte Pair Encoding)\n",
        "\n",
        "En la secci칩n anterior aprendimos a separar por espacios. Sin embargo, palabras como \"like\", \"liker\" y \"lovely\" comparten ra칤ces. El algoritmo BPE busca pares de caracteres que aparecen juntos frecuentemente y los fusiona en un nuevo \"token\".\n",
        "\n",
        "Esto permite que el modelo aprenda s칤labas y morfemas en lugar de palabras enteras, lo cual es mucho m치s eficiente.\n",
        "\n",
        "### 1. Preparaci칩n del Texto y An치lisis de Caracteres\n",
        "Primero, veamos qu칠 caracteres 칰nicos componen nuestro texto."
      ],
      "metadata": {
        "id": "rEKlhocx9M3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducci칩n a Byte Pair Encoding (BPE)\n",
        "\n",
        "Hasta ahora, hemos tokenizado el texto dividi칠ndolo por **espacios** (palabras completas) o por **caracteres** (letras individuales). Sin embargo, los modelos de lenguaje modernos (como GPT-3, GPT-4 o Llama) utilizan un enfoque intermedio llamado **Subword Tokenization** (Tokenizaci칩n por subpalabras), y el algoritmo m치s famoso para esto es **BPE (Byte Pair Encoding)**.\n",
        "\n",
        "### 쮺u치l es el problema que resuelve?\n",
        "\n",
        "1.  **Tokenizaci칩n por palabras:** Si entrenas un modelo con la palabra \"correr\", y luego ve la palabra \"corriendo\", pensar치 que son dos cosas totalmente distintas si no est치n en su vocabulario. El diccionario se vuelve gigantesco e ineficiente.\n",
        "2.  **Tokenizaci칩n por caracteres:** Es muy flexible (puede escribir cualquier palabra), pero las secuencias se vuelven largu칤simas y el modelo pierde el contexto del significado global de la palabra.\n",
        "\n",
        "### La Soluci칩n BPE: La Estrategia de los \"Legos\"\n",
        "\n",
        "Imagina que BPE es como jugar con Legos. En lugar de tener una pieza 칰nica para cada objeto del universo (una pieza \"casa\", una pieza \"perro\"), tienes bloques peque침os que se pueden unir.\n",
        "\n",
        "* Al principio, BPE ve todo como piezas sueltas (letras individuales): `['c', 'o', 'r', 'r', 'e', 'r']`.\n",
        "* Luego, nota que la **'r'** y la **'e'** aparecen juntas muy frecuentemente. Decide fusionarlas en una pieza nueva: **'re'**.\n",
        "* Despu칠s, nota que **'er'** es muy com칰n al final de las palabras. Crea el token **'er'**.\n",
        "\n",
        "Al final, el modelo aprende que palabras como \"correr\", \"comer\" y \"beber\" comparten la terminaci칩n **\"er\"**."
      ],
      "metadata": {
        "id": "UG0VvigT-6sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto con muchas repeticiones para probar el algoritmo\n",
        "text = 'like liker love lovely hug hugs hugging hearts'\n",
        "\n",
        "# Obtenemos los caracteres 칰nicos convirtiendo a set (conjunto) y luego a lista\n",
        "chars = list(set(text))\n",
        "chars.sort() # Ordenamos alfab칠ticamente para mantener consistencia\n",
        "\n",
        "print(f\"Vocabulario inicial (caracteres): {chars}\")\n",
        "\n",
        "# Vemos la frecuencia de cada car치cter\n",
        "for l in chars:\n",
        "    print(f'\"{l}\" aparece {text.count(l)} veces.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0BurVH_9NV9",
        "outputId": "ea2a38e0-b999-404b-ff2c-10cb42a35e86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario inicial (caracteres): [' ', 'a', 'e', 'g', 'h', 'i', 'k', 'l', 'n', 'o', 'r', 's', 't', 'u', 'v', 'y']\n",
            "\" \" aparece 7 veces.\n",
            "\"a\" aparece 1 veces.\n",
            "\"e\" aparece 5 veces.\n",
            "\"g\" aparece 5 veces.\n",
            "\"h\" aparece 4 veces.\n",
            "\"i\" aparece 3 veces.\n",
            "\"k\" aparece 2 veces.\n",
            "\"l\" aparece 5 veces.\n",
            "\"n\" aparece 1 veces.\n",
            "\"o\" aparece 2 veces.\n",
            "\"r\" aparece 2 veces.\n",
            "\"s\" aparece 2 veces.\n",
            "\"t\" aparece 1 veces.\n",
            "\"u\" aparece 3 veces.\n",
            "\"v\" aparece 2 veces.\n",
            "\"y\" aparece 1 veces.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Crear el Vocabulario Inicial y la Lista de Tokens\n",
        "El vocabulario inicial mapea cada car치cter a un 칤ndice num칠rico (o simplemente lo guardamos como lista por ahora). Tambi칠n convertimos nuestro texto original en una lista de caracteres individuales."
      ],
      "metadata": {
        "id": "YI64WnJZ9aqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un vocabulario inicial (diccionario)\n",
        "# Mapeamos el token (caracter) a su 칤ndice en la lista 'chars'\n",
        "vocab = {char: idx for idx, char in enumerate(chars)}\n",
        "print(\"Vocabulario Inicial:\", vocab)\n",
        "\n",
        "# El texto debe ser una lista mutable, no un string\n",
        "# Cada elemento es un token\n",
        "origtext = list(text)\n",
        "\n",
        "print(f\"\\nTexto original (string): {text}\")\n",
        "print(f\"Texto tokenizado (lista): {origtext}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZTAkv_l9a9c",
        "outputId": "296bbcbe-2f0e-4a56-d1d9-603bf855473c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario Inicial: {' ': 0, 'a': 1, 'e': 2, 'g': 3, 'h': 4, 'i': 5, 'k': 6, 'l': 7, 'n': 8, 'o': 9, 'r': 10, 's': 11, 't': 12, 'u': 13, 'v': 14, 'y': 15}\n",
            "\n",
            "Texto original (string): like liker love lovely hug hugs hugging hearts\n",
            "Texto tokenizado (lista): ['l', 'i', 'k', 'e', ' ', 'l', 'i', 'k', 'e', 'r', ' ', 'l', 'o', 'v', 'e', ' ', 'l', 'o', 'v', 'e', 'l', 'y', ' ', 'h', 'u', 'g', ' ', 'h', 'u', 'g', 's', ' ', 'h', 'u', 'g', 'g', 'i', 'n', 'g', ' ', 'h', 'e', 'a', 'r', 't', 's']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Definici칩n de Funciones Principales\n",
        "\n",
        "Para automatizar el proceso, necesitamos tres funciones clave:\n",
        "1.  **`get_pair_stats`**: Cuenta la frecuencia de cada par de tokens adyacentes.\n",
        "2.  **`update_vocab`**: Encuentra el par ganador y lo agrega al vocabulario.\n",
        "3.  **`generate_new_token_seq`**: Reescribe el texto fusionando el par ganador."
      ],
      "metadata": {
        "id": "41ax3KJf9oH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# > -----------------------------------\n",
        "def get_pair_stats(text2pair):\n",
        "    \"\"\"Cuenta cu치ntas veces aparece cada par de tokens consecutivos.\"\"\"\n",
        "    token_pairs = dict()\n",
        "\n",
        "    # Recorremos los tokens hasta el pen칰ltimo\n",
        "    for i in range(len(text2pair)-1):\n",
        "\n",
        "        # Creamos el par (tupla del token actual y el siguiente)\n",
        "        pair = (text2pair[i], text2pair[i+1])\n",
        "\n",
        "        # Incrementamos frecuencia\n",
        "        if pair in token_pairs:\n",
        "            token_pairs[pair] += 1\n",
        "        else: # inicializar\n",
        "            token_pairs[pair] = 1\n",
        "\n",
        "    return token_pairs\n",
        "# ----------------------------------- <\n",
        "\n",
        "# > -----------------------------------\n",
        "def update_vocab(token_pairs, vocab):\n",
        "    \"\"\"Encuentra el par m치s frecuente y lo a침ade al vocabulario.\"\"\"\n",
        "\n",
        "    # Encontrar el par con la m치xima frecuencia\n",
        "    # max() busca la clave con el valor m치s alto en el diccionario\n",
        "    most_frequent_pair = max(token_pairs, key=token_pairs.get)\n",
        "\n",
        "    # El nuevo token ser치 la concatenaci칩n del par (ej: 'l' + 'i' = 'li')\n",
        "    newtok = most_frequent_pair[0] + most_frequent_pair[1]\n",
        "\n",
        "    # Actualizamos el vocabulario\n",
        "    # Asignamos un nuevo ID (simplemente el tama침o actual del vocab + 1 o similar)\n",
        "    next_id = len(vocab)\n",
        "    vocab[newtok] = next_id\n",
        "\n",
        "    print(f\"--> Fusionando par: {most_frequent_pair} en nuevo token: '{newtok}'\")\n",
        "\n",
        "    # Devolvemos el vocabulario actualizado, el par que se fusion칩 y el nuevo token resultante\n",
        "    return vocab, most_frequent_pair, newtok\n",
        "# ----------------------------------- <\n",
        "\n",
        "# > -----------------------------------\n",
        "def generate_new_token_seq(prevtext, pair_to_merge, newtoken_str):\n",
        "    \"\"\"Reemplaza las ocurrencias del par separado por el nuevo token fusionado.\"\"\"\n",
        "\n",
        "    # Inicializar la nueva lista de texto\n",
        "    newtext = []\n",
        "\n",
        "    # Bucle a trav칠s de la lista\n",
        "    i = 0\n",
        "    while i < (len(prevtext)-1):\n",
        "\n",
        "        # Probamos si el par de este elemento y el siguiente coinciden con el par ganador\n",
        "        if prevtext[i] == pair_to_merge[0] and prevtext[i+1] == pair_to_merge[1]:\n",
        "            # 춰Coincidencia! A침adimos el token fusionado\n",
        "            newtext.append(newtoken_str)\n",
        "            # Saltamos el siguiente car치cter porque ya lo fusionamos\n",
        "            i += 2\n",
        "\n",
        "        # No es un par\n",
        "        else:\n",
        "            newtext.append(prevtext[i])\n",
        "            # Nos movemos al siguiente car치cter normalmente\n",
        "            i += 1\n",
        "\n",
        "    # El c칩digo original del video a veces omite el 칰ltimo car치cter si no formaba parte de un par\n",
        "    # Esta l칩gica asegura que si sobra un elemento al final, se agregue.\n",
        "    if i == len(prevtext) - 1:\n",
        "        newtext.append(prevtext[i])\n",
        "\n",
        "    return newtext\n",
        "# ----------------------------------- <"
      ],
      "metadata": {
        "id": "jlrQ-grj9oaU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Ejercicio Principal: El Bucle de Entrenamiento\n",
        "Ahora vamos a ejecutar el algoritmo iterativamente hasta alcanzar un tama침o de vocabulario deseado. Observa c칩mo la lista `updated_text` se va haciendo m치s corta a medida que los caracteres se fusionan en palabras."
      ],
      "metadata": {
        "id": "ZiFjoL1M-E64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-inicializamos el vocabulario para asegurar limpieza\n",
        "vocab = {char: idx for idx, char in enumerate(chars)}\n",
        "\n",
        "# 쮺u치ntos tokens queremos en total?\n",
        "vocab_size = 25\n",
        "\n",
        "# Hacemos una copia del texto original\n",
        "updated_text = origtext.copy()\n",
        "\n",
        "# Bucle principal\n",
        "while len(vocab) < vocab_size:\n",
        "\n",
        "    # 1. Obtener estad칤sticas de pares\n",
        "    stats = get_pair_stats(updated_text)\n",
        "\n",
        "    if not stats: # Si no hay m치s pares para fusionar, rompemos el ciclo\n",
        "        break\n",
        "\n",
        "    # 2. Actualizar el vocabulario con el mejor par\n",
        "    # Nota: modifiqu칠 update_vocab para devolver 3 valores para mayor claridad\n",
        "    vocab, best_pair, new_token_str = update_vocab(stats, vocab)\n",
        "\n",
        "    # 3. Generar la nueva secuencia de texto con el token fusionado\n",
        "    updated_text = generate_new_token_seq(updated_text, best_pair, new_token_str)\n",
        "\n",
        "print(\"\\n--- Proceso Terminado ---\")\n",
        "print(f\"Tama침o final del vocabulario: {len(vocab)}\")\n",
        "print(f\"Texto final tokenizado: {updated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLVTWdPw-FRr",
        "outputId": "05b298aa-0e7b-483d-dfe6-9cd3280af8c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Fusionando par: (' ', 'h') en nuevo token: ' h'\n",
            "--> Fusionando par: (' ', 'l') en nuevo token: ' l'\n",
            "--> Fusionando par: (' h', 'u') en nuevo token: ' hu'\n",
            "--> Fusionando par: (' hu', 'g') en nuevo token: ' hug'\n",
            "--> Fusionando par: ('i', 'k') en nuevo token: 'ik'\n",
            "--> Fusionando par: ('ik', 'e') en nuevo token: 'ike'\n",
            "--> Fusionando par: (' l', 'o') en nuevo token: ' lo'\n",
            "--> Fusionando par: (' lo', 'v') en nuevo token: ' lov'\n",
            "--> Fusionando par: (' lov', 'e') en nuevo token: ' love'\n",
            "\n",
            "--- Proceso Terminado ---\n",
            "Tama침o final del vocabulario: 25\n",
            "Texto final tokenizado: ['l', 'ike', ' l', 'ike', 'r', ' love', ' love', 'l', 'y', ' hug', ' hug', 's', ' hug', 'g', 'i', 'n', 'g', ' h', 'e', 'a', 'r', 't', 's']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Resultado Final\n",
        "Observemos el vocabulario que ha aprendido la m치quina. Deber칤a contener palabras completas o partes significativas (como 'ing' o 'er') adem치s de los caracteres individuales."
      ],
      "metadata": {
        "id": "gxKEfYHW-Sdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir el vocabulario final ordenado por 칤ndice (si usamos 칤ndices secuenciales) o simplemente las claves\n",
        "print(\"Vocabulario generado:\")\n",
        "for token, idx in vocab.items():\n",
        "    print(f\"Token: '{token}' \\t ID: {idx}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSnwfmjj-SsU",
        "outputId": "36e79ca1-08bf-4149-c661-54a494cbf909"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario generado:\n",
            "Token: ' ' \t ID: 0\n",
            "Token: 'a' \t ID: 1\n",
            "Token: 'e' \t ID: 2\n",
            "Token: 'g' \t ID: 3\n",
            "Token: 'h' \t ID: 4\n",
            "Token: 'i' \t ID: 5\n",
            "Token: 'k' \t ID: 6\n",
            "Token: 'l' \t ID: 7\n",
            "Token: 'n' \t ID: 8\n",
            "Token: 'o' \t ID: 9\n",
            "Token: 'r' \t ID: 10\n",
            "Token: 's' \t ID: 11\n",
            "Token: 't' \t ID: 12\n",
            "Token: 'u' \t ID: 13\n",
            "Token: 'v' \t ID: 14\n",
            "Token: 'y' \t ID: 15\n",
            "Token: ' h' \t ID: 16\n",
            "Token: ' l' \t ID: 17\n",
            "Token: ' hu' \t ID: 18\n",
            "Token: ' hug' \t ID: 19\n",
            "Token: 'ik' \t ID: 20\n",
            "Token: 'ike' \t ID: 21\n",
            "Token: ' lo' \t ID: 22\n",
            "Token: ' lov' \t ID: 23\n",
            "Token: ' love' \t ID: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGqV3-Bx-e6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 游늴 쮹PE reduce el vocabulario? La Paradoja de la Tokenizaci칩n\n",
        "\n",
        "Es com칰n confundirse, pero la respuesta t칠cnica es: **El vocabulario CRECE, pero la longitud del texto se REDUCE**.\n",
        "\n",
        "Para entenderlo, debemos distinguir entre las dos dimensiones principales:\n",
        "\n",
        "### 1. El Vocabulario (Tu caja de herramientas)\n",
        "**Estado:** <span style=\"color:green\">**AUMENTA 游늳**</span>\n",
        "\n",
        "El vocabulario es la lista de tokens 칰nicos que tu modelo conoce.\n",
        "* **Al inicio:** Tienes un vocabulario peque침o (solo letras individuales: `a, b, c...`).\n",
        "* **Durante BPE:** El algoritmo *agrega* nuevos tokens fusionados (`er`, `ing`, `love`).\n",
        "* **Resultado:** Terminas con **m치s tokens 칰nicos** en tu diccionario que al principio.\n",
        "\n",
        "### 2. La Secuencia (El mensaje a procesar)\n",
        "**Estado:** <span style=\"color:red\">**SE REDUCE 游늴**</span>\n",
        "\n",
        "Esto se refiere a la cantidad de tokens que la IA necesita leer para entender una frase (Compresi칩n).\n",
        "* **Antes (Caracteres):** Para leer **\"like\"**, la IA procesaba **4 tokens** (`['l', 'i', 'k', 'e']`).\n",
        "* **Despu칠s (BPE):** Si aprendi칩 el token **\"like\"**, ahora solo procesa **1 token** (`['like']`).\n",
        "\n",
        "---\n",
        "\n",
        "### 游늵 Resumen Visual\n",
        "\n",
        "| Concepto | Inicio (Solo Caracteres) | Final (Con BPE) | Resultado |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Vocabulario**<br>*(Piezas 칰nicas)* | `['l', 'i', 'k', 'e'...]`<br>*(Pocas piezas, muy b치sicas)* | `['l', 'i', 'k', 'e', 'like'...]`<br>*(M치s piezas, m치s complejas)* | **CRECE** (M치s memoria necesaria) |\n",
        "| **Secuencia**<br>*(Longitud del texto)* | `['l', 'i', 'k', 'e']`<br>*(4 pasos para leer)* | `['like']`<br>*(1 paso para leer)* | **SE REDUCE** (M치s velocidad de c칩mputo) |\n",
        "\n",
        "> **Conclusi칩n:** Cambiamos un diccionario un poco m치s pesado a cambio de una lectura mucho m치s r치pida y eficiente."
      ],
      "metadata": {
        "id": "qsALaesUCI0T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBpRMKYMCJIM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}