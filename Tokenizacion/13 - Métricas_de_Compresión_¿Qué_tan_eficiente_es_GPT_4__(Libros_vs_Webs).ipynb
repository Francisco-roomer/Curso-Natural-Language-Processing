{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Configuraci칩n y Herramientas\n",
        "Importamos `requests` para navegar por internet, `tiktoken` para tokenizar, y `urllib` para limpiar los nombres de las webs."
      ],
      "metadata": {
        "id": "AhZhuYL_Y1Fx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 1: Analizando Libros Cl치sicos (Proyecto Gutenberg)\n",
        "\n",
        "Vamos a descargar 10 libros cl치sicos y calcular su **Ratio de Compresi칩n**.\n",
        "La f칩rmula es: `Caracteres / Tokens`.\n",
        "* Un n칰mero alto (ej: 4.0) significa buena compresi칩n (el modelo lee r치pido).\n",
        "* Un n칰mero bajo (ej: 1.5) significa mala compresi칩n.\n",
        "\n",
        "*Nota: La URL se construye din치micamente usando el c칩digo del libro.*"
      ],
      "metadata": {
        "id": "WOPM8IyOY8qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for getting text data off the web\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "# strings\n",
        "import string\n",
        "\n",
        "# !pip install tiktoken # Descomentar si es necesario\n",
        "import tiktoken\n",
        "\n",
        "# GPT-4's tokenizer\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')"
      ],
      "metadata": {
        "id": "bZy7fPDkY1YV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9LXtaJMnY5MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 1: Analizando Libros Cl치sicos (Proyecto Gutenberg)\n",
        "\n",
        "Vamos a descargar 10 libros cl치sicos y calcular su **Ratio de Compresi칩n**.\n",
        "La f칩rmula es: `Caracteres / Tokens`.\n",
        "* Un n칰mero alto (ej: 4.0) significa buena compresi칩n (el modelo lee r치pido).\n",
        "* Un n칰mero bajo (ej: 1.5) significa mala compresi칩n.\n",
        "\n",
        "*Nota: La URL se construye din치micamente usando el c칩digo del libro.*"
      ],
      "metadata": {
        "id": "GOeABxqBZI1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: The books\n",
        "\n",
        "# all books have the same url format;\n",
        "# they are unique by numerical code\n",
        "baseurl = 'https://www.gutenberg.org/cache/epub/'\n",
        "\n",
        "bookurls = [\n",
        "    # code       title\n",
        "    ['84',    'Frankenstein'    ],\n",
        "    ['64317', 'GreatGatsby'     ],\n",
        "    ['11',    'AliceWonderland' ],\n",
        "    ['1513',  'RomeoJuliet'     ],\n",
        "    ['76',    'HuckFinn'        ],\n",
        "    ['219',   'HeartDarkness'   ],\n",
        "    ['2591',  'GrimmsTales'     ],\n",
        "    ['2148',  'EdgarAllenPoe'   ],\n",
        "    ['36',    'WarOfTheWorlds'  ],\n",
        "    ['829',   'GulliversTravels']\n",
        "]\n",
        "\n",
        "\n",
        "print('  Book title     |  Chars  |  Tokens | Compression')\n",
        "print('-'*50)\n",
        "\n",
        "for code, title in bookurls:\n",
        "\n",
        "  # get the text\n",
        "  # Construimos la URL completa (ej: .../84/pg84.txt)\n",
        "  fullurl = f'{baseurl}{code}/pg{code}.txt'\n",
        "\n",
        "  try:\n",
        "      response = requests.get(fullurl)\n",
        "      response.encoding = 'utf-8' # Asegurar codificaci칩n correcta\n",
        "      text = response.text\n",
        "  except:\n",
        "      text = \"\" # Si falla, texto vac칤o\n",
        "\n",
        "  num_chars = len(text)\n",
        "\n",
        "  # tokenize\n",
        "  if num_chars > 0:\n",
        "      tokens = tokenizer.encode(text)\n",
        "      num_tokens = len(tokens)\n",
        "  else:\n",
        "      num_tokens = 1 # Evitar divisi칩n por cero\n",
        "\n",
        "  # compression ratio (Caracteres por Token)\n",
        "  compress = num_chars / num_tokens\n",
        "\n",
        "  print(f'{title:16} | {num_chars:>7,d} | {num_tokens:>7,d} |  {compress:>3.2f} chars/token')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T333w1NY86F",
        "outputId": "bc42a4f9-0ee5-4eb3-da31-2c82a4a7a3ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Book title     |  Chars  |  Tokens | Compression\n",
            "--------------------------------------------------\n",
            "Frankenstein     | 446,544 | 102,419 |  4.36 chars/token\n",
            "GreatGatsby      | 296,858 |  70,343 |  4.22 chars/token\n",
            "AliceWonderland  | 167,674 |  41,457 |  4.04 chars/token\n",
            "RomeoJuliet      | 167,429 |  43,761 |  3.83 chars/token\n",
            "HuckFinn         | 602,714 | 159,125 |  3.79 chars/token\n",
            "HeartDarkness    | 232,885 |  56,483 |  4.12 chars/token\n",
            "GrimmsTales      | 549,736 | 137,252 |  4.01 chars/token\n",
            "EdgarAllenPoe    | 632,136 | 144,315 |  4.38 chars/token\n",
            "WarOfTheWorlds   | 363,420 |  84,580 |  4.30 chars/token\n",
            "GulliversTravels | 611,742 | 143,560 |  4.26 chars/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 2: Analizando Sitios Web (C칩digo HTML)\n",
        "\n",
        "Ahora haremos lo mismo con sitios web.\n",
        "**Hip칩tesis:** Como las webs tienen mucho c칩digo HTML (`<div>`, `<script>`, `href=...`), esperamos que la compresi칩n sea **peor** (m치s baja) que en los libros, porque el c칩digo tiene muchos s칤mbolos que rompen los tokens."
      ],
      "metadata": {
        "id": "okeGWjvmZU4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: Repeat with websites\n",
        "\n",
        "weburls = [\n",
        "    'http://python.org/',\n",
        "    'https://pytorch.org/',\n",
        "    'https://en.wikipedia.org/wiki/List_of_English_words_containing_Q_not_followed_by_U',\n",
        "    'https://sudoku.com/',\n",
        "    'https://reddit.com/',\n",
        "    'https://visiteurope.com/en/',\n",
        "    'https://sincxpress.com/',\n",
        "    'https://openai.com/',\n",
        "    'https://theuselessweb.com/',\n",
        "    'https://maps.google.com/',\n",
        "    'https://pigeonsarentreal.co.uk/',\n",
        "]\n",
        "\n",
        "\n",
        "print('    Website        |  Chars  |  Tokens | Compression')\n",
        "print('-'*53)\n",
        "\n",
        "for url in weburls:\n",
        "\n",
        "  # get the text\n",
        "  try:\n",
        "      # Usamos headers para simular un navegador real y evitar bloqueos (403 Forbidden)\n",
        "      headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "      text = requests.get(url, headers=headers).text\n",
        "  except:\n",
        "      text = \"Error\"\n",
        "\n",
        "  num_chars = len(text)\n",
        "\n",
        "  # tokenize\n",
        "  if num_chars > 0:\n",
        "      tokens = tokenizer.encode(text)\n",
        "      num_tokens = len(tokens)\n",
        "  else:\n",
        "      num_tokens = 1\n",
        "\n",
        "  # compression ratio\n",
        "  compress = num_chars / num_tokens\n",
        "\n",
        "  # Usamos urlparse para obtener solo el nombre limpio del sitio (hostname)\n",
        "  hostname = urlparse(url).hostname\n",
        "  if hostname is None: hostname = \"unknown\"\n",
        "\n",
        "  print(f'{hostname[:18]:18} | {num_chars:>7,d} | {num_tokens:>7,d} |  {compress:>3.2f} chars/token')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLIye8qMZVHV",
        "outputId": "e3c1db4a-f681-4380-f932-a6f6ff7e80a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Website        |  Chars  |  Tokens | Compression\n",
            "-----------------------------------------------------\n",
            "python.org         |  48,288 |  12,236 |  3.95 chars/token\n",
            "pytorch.org        | 184,926 |  56,827 |  3.25 chars/token\n",
            "en.wikipedia.org   | 158,792 |  50,812 |  3.13 chars/token\n",
            "sudoku.com         | 140,695 |  51,330 |  2.74 chars/token\n",
            "reddit.com         |   1,522 |     395 |  3.85 chars/token\n",
            "visiteurope.com    | 405,838 | 155,130 |  2.62 chars/token\n",
            "sincxpress.com     |  25,580 |   6,843 |  3.74 chars/token\n",
            "openai.com         |  11,639 |   6,424 |  1.81 chars/token\n",
            "theuselessweb.com  |   4,756 |   1,329 |  3.58 chars/token\n",
            "maps.google.com    | 214,606 | 109,076 |  1.97 chars/token\n",
            "pigeonsarentreal.c |   7,300 |   2,051 |  3.56 chars/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 3: Analizando la Librer칤a `string`\n",
        "\n",
        "Python tiene una librer칤a llamada `string` que contiene constantes como \"todas las letras min칰sculas\" o \"todos los signos de puntuaci칩n\".\n",
        "Vamos a ver qu칠 tan eficientes son estos grupos de caracteres.\n",
        "* Los d칤gitos suelen ser eficientes.\n",
        "* La puntuaci칩n suele ser ineficiente (1 caracter = 1 token)."
      ],
      "metadata": {
        "id": "LE_gBxOSZbnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: Using the 'string' library\n",
        "\n",
        "# e.g., inspect available attributes\n",
        "# string.__dict__['__doc__']\n",
        "# string.ascii_lowercase\n",
        "\n",
        "print('  Attribute     |  Chars  |  Tokens | Compression')\n",
        "print('-'*50)\n",
        "\n",
        "for k,v in string.__dict__.items():\n",
        "  # Check if the attribute is a string (ignoring functions or classes)\n",
        "  if isinstance(v, str):\n",
        "\n",
        "    # get the text\n",
        "    text = v\n",
        "    num_chars = len(text)\n",
        "\n",
        "    # tokenize\n",
        "    tokens = tokenizer.encode(text)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    # compression ratio\n",
        "    if num_tokens > 0:\n",
        "        compress = num_chars / num_tokens\n",
        "    else:\n",
        "        compress = 0\n",
        "\n",
        "    # print the results (Omitimos atributos vac칤os o muy cortos para limpiar la salida)\n",
        "    if num_chars > 5:\n",
        "        print(f'{k:15} | {num_chars:>7,d} | {num_tokens:>7,d} |  {compress:>3.2f} chars/token')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgl1t48FZb3k",
        "outputId": "79d8ed2c-79cd-4a93-ca38-b44641e7e8a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attribute     |  Chars  |  Tokens | Compression\n",
            "--------------------------------------------------\n",
            "__name__        |       6 |       1 |  6.00 chars/token\n",
            "__doc__         |     622 |     109 |  5.71 chars/token\n",
            "__file__        |      29 |       8 |  3.62 chars/token\n",
            "__cached__      |      54 |      17 |  3.18 chars/token\n",
            "whitespace      |       6 |       4 |  1.50 chars/token\n",
            "ascii_lowercase |      26 |       1 |  26.00 chars/token\n",
            "ascii_uppercase |      26 |       1 |  26.00 chars/token\n",
            "ascii_letters   |      52 |       2 |  26.00 chars/token\n",
            "digits          |      10 |       4 |  2.50 chars/token\n",
            "hexdigits       |      22 |       7 |  3.14 chars/token\n",
            "octdigits       |       8 |       3 |  2.67 chars/token\n",
            "punctuation     |      32 |      21 |  1.52 chars/token\n",
            "printable       |     100 |      31 |  3.23 chars/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXNK7Gw5ZeTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explicaci칩n: M칠tricas de Compresi칩n y Eficiencia\n",
        "\n",
        "En esta secci칩n estamos calculando el **Ratio de Compresi칩n** (Efficiency Score).\n",
        "La f칩rmula es simple:\n",
        "$$\\text{Eficiencia} = \\frac{\\text{Caracteres totales}}{\\text{Tokens totales}}$$\n",
        "\n",
        "### 쯇or qu칠 nos importa esto?\n",
        "\n",
        "OpenAI (y la mayor칤a de las APIs de IA) te cobran por **token**, no por palabra ni por car치cter.\n",
        "* **Alta Eficiencia (ej: 4.0):** Significa que cada token transporta mucha informaci칩n (muchas letras). Es **barato**.\n",
        "* **Baja Eficiencia (ej: 1.2):** Significa que necesitas muchos tokens para decir poco. Es **caro**.\n",
        "\n",
        "### An치lisis de los Resultados\n",
        "\n",
        "#### 1. Libros (Lenguaje Natural)\n",
        "Ver치s que los libros cl치sicos (*Frankenstein*, *Alice in Wonderland*) tienen una eficiencia alta (alrededor de **3.5 a 4.5** caracteres por token).\n",
        "* **Raz칩n:** El algoritmo BPE fue entrenado con mucho texto en ingl칠s. Sabe comprimir palabras comunes como \"the\", \"ing\", \"tion\" en un solo token.\n",
        "\n",
        "#### 2. Sitios Web (C칩digo HTML y Ruido)\n",
        "Ver치s que la eficiencia cae dr치sticamente (alrededor de **1.5 a 2.5**).\n",
        "* **Raz칩n:** El c칩digo HTML est치 lleno de s칤mbolos que rompen los tokens (`<`, `>`, `/`, `=`, `\"`).\n",
        "* **Ejemplo:** La palabra `class` es un token. Pero en HTML suele aparecer como `class=\"estilo\"`. Las comillas y el igual obligan al tokenizador a separar todo en pedazos peque침os.\n",
        "\n",
        "#### 3. Librer칤a String (Categor칤as Puras)\n",
        "Aqu칤 vemos los extremos:\n",
        "* **D칤gitos:** Suelen ser eficientes o neutros.\n",
        "* **Puntuaci칩n (`punctuation`):** Es lo peor. Generalmente **1 car치cter = 1 token**.\n",
        "\n",
        "### 游눠 Lecci칩n para tu Negocio de Automatizaci칩n\n",
        "Si vas a automatizar la lectura de webs o bases de datos:\n",
        "**Limpia el c칩digo antes de enviarlo a GPT.**\n",
        "Si env칤as HTML crudo (`<div><h1>Hola</h1></div>`), est치s pagando por tokens de etiquetas que no aportan significado. Si limpias el texto (\"Hola\"), ahorras dinero y el modelo entiende mejor."
      ],
      "metadata": {
        "id": "Cim4-R7OZ2Gt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_RT3KvZZ2i1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}