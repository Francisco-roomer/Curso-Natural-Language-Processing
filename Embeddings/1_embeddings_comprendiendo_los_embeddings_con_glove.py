# -*- coding: utf-8 -*-
"""1_embeddings_comprendiendo_los_embeddings_con_Glove.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11EyKnQttYbneyWlnazVO77HBn8Lmi2DE

# Introducción a los Embeddings (Word Embeddings)

En esta sección, pasamos de los "tokens" (enteros discretos) a los "embeddings" (vectores continuos). Utilizaremos **GloVe** (Global Vectors for Word Representation), un modelo pre-entrenado muy popular.

Específicamente, cargaremos una versión entrenada con **Wikipedia y Gigaword** que representa cada palabra como un vector de **50 dimensiones**.

> **Nota:** Si tienes errores de importación, descomenta la línea de `!pip install` y reinicia tu entorno.
"""

# Descargar un modelo GloVe pequeño (Wikipedia + Gigaword, 50D)
# NOTA: Si obtienes errores al importar, ejecuta la siguiente línea !pip...,
# luego reinicia tu sesión (desde el menú Runtime) y vuelve a comentar la línea pip.
!pip install gensim

import gensim.downloader as api

# Cargamos el modelo (esto puede tardar un poco la primera vez)
glove = api.load('glove-wiki-gigaword-50')

"""## Configuración de Visualización

Importamos las librerías estándar para cálculo numérico (`numpy`) y visualización (`matplotlib`). También configuramos los gráficos para que se rendericen en formato SVG (más nítidos).
"""

import numpy as np
import scipy
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec

# Gráficos en formato SVG
import matplotlib_inline.backend_inline
matplotlib_inline.backend_inline.set_matplotlib_formats('svg')

"""## Explorando el Objeto GloVe

Primero, inspeccionemos qué métodos y propiedades tiene el objeto `glove` que acabamos de descargar. También verificaremos el tamaño del vocabulario.
"""

# Verificar propiedades y métodos
dir(glove)

print(f'El diccionario contiene {len(glove.key_to_index.keys())} items.')

# Mostramos los primeros 50 elementos del diccionario
list(glove.key_to_index.keys())[:50]

"""## Exploración Aleatoria del Vocabulario

Es buena práctica mirar qué hay dentro del "cerebro" del modelo. Seleccionamos índices al azar para ver qué palabras (o símbolos) existen en el vocabulario.
"""

# Imprimir 10 palabras al azar
for idx in np.random.randint(0, len(glove.key_to_index), 10):
  print(f'El índice {idx:>6} es "{glove.index_to_key[idx]}"')

"""## Distribución de Longitud de Palabras

¿Son largas o cortas las palabras que conoce el modelo?
Este bloque analiza la morfología del vocabulario contando los caracteres de cada token. Usamos una escala logarítmica en el eje Y porque la cantidad de palabras cortas suele ser exponencialmente mayor que las largas.
"""

# Distribución de longitudes de caracteres de los tokens
token_lengths = np.zeros(len(glove.key_to_index.keys()), dtype=int)

for idx, word in enumerate(glove.key_to_index.keys()):
  token_lengths[idx] = len(word)

# Conteos para el gráfico de barras
uniqVals, uniqCounts = np.unique(token_lengths, return_counts=True)

# Visualizar la distribución de longitudes
plt.figure(figsize=(12, 4))
plt.bar(uniqVals,(uniqCounts), width=uniqVals[1]-uniqVals[0], facecolor=[.9, .7, .9], edgecolor='k')
plt.gca().set(xlabel='Longitud de palabra (num caracteres)', ylabel='Conteo (log)')
plt.show()

# Distribución de longitudes de caracteres de los tokens
token_lengths = np.zeros(len(glove.key_to_index.keys()), dtype=int)

for idx, word in enumerate(glove.key_to_index.keys()):
  token_lengths[idx] = len(word)

# Conteos para el gráfico de barras
uniqVals, uniqCounts = np.unique(token_lengths, return_counts=True)

# Visualizar la distribución de longitudes
plt.figure(figsize=(12, 4))
plt.bar(uniqVals, np.log(uniqCounts), width=uniqVals[1]-uniqVals[0], facecolor=[.9, .7, .9], edgecolor='k')
plt.gca().set(xlabel='Longitud de palabra (num caracteres)', ylabel='Conteo (log)')
plt.show()

"""## La Matriz de Embeddings

Aquí visualizamos **todo el conocimiento del modelo**.
La matriz tiene una forma de `(400,000 palabras x 50 dimensiones)`.
El mapa de calor (heatmap) muestra cómo cada palabra se representa mediante una combinación única de valores numéricos entre -1 y 1 (aproximadamente).
"""

# Tamaño de la matriz de embeddings
print(f'La matriz de embeddings es {glove.vectors.shape}')

print(f'La palabra "apple" tiene el índice #{glove.key_to_index["apple"]}')

# También se puede acceder así:
glove.get_index('apple')

plt.figure(figsize=(12, 4))
# Transponemos (.T) para visualizar mejor: Eje X = Palabras, Eje Y = Dimensiones
plt.imshow(glove.vectors.T, vmin=-1, vmax=1, aspect='auto')
plt.gca().set(ylabel='Dimensión', xlabel='Índice de palabra', title='Matriz de Embeddings')
plt.colorbar(pad=.01)
plt.show()



"""## Propiedades Estadísticas de los Vectores

Analizamos la media y la desviación estándar de los vectores. Esto nos ayuda a entender si los vectores están normalizados o si ciertas palabras tienen representaciones numéricas más "intensas" que otras.
"""

# Media y desviación estándar a través de cada dimensión de embedding
emb_mean = glove.vectors.mean(axis=1)
emb_std  = glove.vectors.std(axis=1)

# Seaborn tiene rutinas de visualización agradables
import seaborn as sns
import pandas as pd # Seaborn requiere pandas dataframes

df = pd.DataFrame(np.vstack((emb_mean, emb_std)).T, columns=['Mean', 'std'])

sns.jointplot(x='Mean', y='std', data=df, alpha=.2)
plt.show()

"""## Inspección de un Embedding Individual

Vamos a ver cómo "ve" la máquina una palabra específica, por ejemplo: **Banana**.
Para la computadora, "banana" no es una fruta amarilla, es una lista de 50 números flotantes que la ubican en un espacio semántico.
"""

# Elegir una palabra
word = 'banana'

# Obtener su índice en la matriz
wordidx = glove.key_to_index[word]

# Obtener el vector de embedding
thisWordVector = glove.vectors[wordidx, :]

# Inspeccionar el vector
print(f'El vector de embedding para "{word}" es\n {thisWordVector}')

# Manera más fácil de acceder ;)
thisWordVector = glove[word]
print(f'El vector de embedding para "{word}" es\n {thisWordVector}')

# Visualizarlo
plt.figure(figsize=(10, 4))
plt.plot(glove.vectors[wordidx, :], 'ks', markersize=10, markerfacecolor=[.7, .7, .9])

plt.xlabel('Dimensión')
plt.title(f'Vector de embedding para "{word}"')
plt.show()

"""## Relaciones Semánticas y Similitud Coseno

Este es el concepto clave de los embeddings.
Comparamos `banana`, `apple` y `cosmic`.
1.  **Gráfico superior:** Muestra los valores de las 50 dimensiones. Observa cómo las líneas de `banana` y `apple` siguen patrones similares, mientras que `cosmic` es diferente.
2.  **Gráficos inferiores:** Muestra la correlación directa.
    * `banana` vs `apple`: Alta similitud (puntos agrupados en diagonal).
    * `banana` vs `cosmic`: Baja similitud (nube dispersa).
"""

# Elegir tres palabras
word1 = 'banana'
word2 = 'apple'
word3 = 'cosmic'

# Configurar la geometría de los subplots
fig = plt.figure(figsize=(10, 7))
gs = GridSpec(2, 2)
ax0 = fig.add_subplot(gs[0, :])
ax1 = fig.add_subplot(gs[1, 0])
ax2 = fig.add_subplot(gs[1, 1])

# Graficar los embeddings por dimensión
for idx, word in enumerate([word1, word2, word3]):
  ax0.plot(glove[word], 's-', label=word)

ax0.set(xlabel='Dimensión', title='Embeddings', xlim=[-1, glove.vectors.shape[1]+1])
ax0.legend()

# Graficar los embeddings uno contra otro (Similitud)
cossim = glove.similarity(word1, word2)
ax1.plot(glove[word1], glove[word2], 'ko', markerfacecolor=[.9, .7, .7])
ax1.set(xlabel=word1, ylabel=word2, title=f'Similitud Coseno = {cossim:.3f}')

cossim = glove.similarity(word1, word3)
ax2.plot(glove[word1], glove[word3], 'ko', markerfacecolor=[.7, .9, .7])
ax2.set(xlabel=word1, ylabel=word3, title=f'Similitud Coseno = {cossim:.3f}')

# Toques finales
plt.tight_layout()
plt.show()



"""## Búsqueda Semántica y Analogías

Finalmente, usamos el poder del álgebra lineal aplicada al lenguaje.
1.  `most_similar`: Encuentra los vecinos más cercanos en el espacio vectorial.
2.  `doesnt_match`: Encuentra el vector "intruso" o atípico en una lista, calculando cuál tiene la menor similitud promedio con el resto.
"""

# Palabras más similares ("similar" significa alta similitud coseno)
glove.most_similar('fashion', topn=9)

# Una de estas cosas no es como las otras...
lists = [ [ 'apple', 'banana', 'pirate', 'peach' ],
          [ 'apple', 'banana', 'peach', 'kiwi', 'starfruit' ],
          [ 'apple', 'banana', 'pirate', 'peach', 'kiwi', 'starfruit' ],
          [ 'apple', 'banana', 'orange', 'kiwi' ]
        ]

for l in lists:
  print(f'En la lista de palabras {l}:')
  # Nota: most_similar devuelve el promedio del vector si se pasa una lista, aquí buscamos similitud de grupo
  # pero para encontrar el intruso usamos doesnt_match
  print(f'  La palabra más representativa (contextual) podría ser: "{glove.most_similar(l, topn=1)[0][0]}"')
  print(f'  y la palabra que NO encaja es: "{glove.doesnt_match(l)}"\n')

"""# Análisis de Resultados: La "Geometría" del Lenguaje

En esta sección hemos desmitificado qué son realmente los embeddings. A diferencia de los tokens (que son simples números enteros de identificación), los embeddings son **representaciones distribuidas**. Aquí están las conclusiones clave de nuestros experimentos:

### 1. Las Palabras son Coordenadas
Al cargar `glove-wiki-gigaword-50`, hemos convertido cada palabra del diccionario en un punto dentro de un hiper-espacio de 50 dimensiones.
* **El vector:** La lista de 50 números que vimos para "banana" no tiene sentido si miramos número por número. El significado no está en un solo número, sino en el **patrón conjunto** de los 50 números.

### 2. Similitud Visual y Matemática
El gráfico comparativo entre `banana`, `apple` y `cosmic` es la prueba visual de la semántica vectorial:
* **Patrones coincidentes:** Las líneas de "banana" y "apple" subían y bajaban casi en los mismos lugares (dimensiones). Esto indica que comparten atributos (ambas son frutas, son comida, objetos físicos).
* **Patrones divergentes:** La línea de "cosmic" no tenía correlación con las anteriores.
* **Similitud Coseno:** Matemáticamente, calculamos el ángulo entre estos vectores. Un valor cercano a 1 indica que los vectores apuntan a la misma dirección (sinónimos o relacionados); cerca de 0 indica que no tienen relación.

### 3. El Juego de "Encuentra al Intruso" (`doesnt_match`)
Cuando le dimos al modelo la lista `['apple', 'banana', 'pirate', 'peach']`, el modelo realizó una operación geométrica fascinante:
1. Calculó el "centro" (promedio) de todos los vectores.
2. Midió la distancia de cada palabra a ese centro.
3. Identificó que `apple`, `banana` y `peach` estaban agrupadas en una región (la región de "frutas"), mientras que `pirate` estaba muy lejos de ese clúster.

### 4. Limitaciones del Modelo
Aunque GloVe es poderoso, tiene limitaciones visibles en el gráfico de distribución de longitud:
* El modelo conoce ~400,000 palabras.
* Si intentamos buscar una palabra muy moderna (ej. "COVID-19" o jerga de 2024) o mal escrita que no esté en ese vocabulario fijo, el modelo fallará (Error `KeyError`). A diferencia del Tokenizador BPE que vimos antes (que rompe las palabras desconocidas en partes), los modelos de embeddings estáticos como GloVe requieren que la palabra exacta exista en su diccionario.

### Conclusión
Los embeddings permiten a las máquinas entender analogías y relaciones. No entienden el concepto filosófico de una "manzana", pero entienden perfectamente que:
$$\text{Manzana} - \text{Fruta} + \text{Tecnología} \approx \text{Apple (la empresa)}$$
*(Esta es la famosa propiedad de las analogías vectoriales).*# Análisis de Resultados: La "Geometría" del Lenguaje

En esta sección hemos desmitificado qué son realmente los embeddings. A diferencia de los tokens (que son simples números enteros de identificación), los embeddings son **representaciones distribuidas**. Aquí están las conclusiones clave de nuestros experimentos:

### 1. Las Palabras son Coordenadas
Al cargar `glove-wiki-gigaword-50`, hemos convertido cada palabra del diccionario en un punto dentro de un hiper-espacio de 50 dimensiones.
* **El vector:** La lista de 50 números que vimos para "banana" no tiene sentido si miramos número por número. El significado no está en un solo número, sino en el **patrón conjunto** de los 50 números.

### 2. Similitud Visual y Matemática
El gráfico comparativo entre `banana`, `apple` y `cosmic` es la prueba visual de la semántica vectorial:
* **Patrones coincidentes:** Las líneas de "banana" y "apple" subían y bajaban casi en los mismos lugares (dimensiones). Esto indica que comparten atributos (ambas son frutas, son comida, objetos físicos).
* **Patrones divergentes:** La línea de "cosmic" no tenía correlación con las anteriores.
* **Similitud Coseno:** Matemáticamente, calculamos el ángulo entre estos vectores. Un valor cercano a 1 indica que los vectores apuntan a la misma dirección (sinónimos o relacionados); cerca de 0 indica que no tienen relación.

### 3. El Juego de "Encuentra al Intruso" (`doesnt_match`)
Cuando le dimos al modelo la lista `['apple', 'banana', 'pirate', 'peach']`, el modelo realizó una operación geométrica fascinante:
1. Calculó el "centro" (promedio) de todos los vectores.
2. Midió la distancia de cada palabra a ese centro.
3. Identificó que `apple`, `banana` y `peach` estaban agrupadas en una región (la región de "frutas"), mientras que `pirate` estaba muy lejos de ese clúster.

### 4. Limitaciones del Modelo
Aunque GloVe es poderoso, tiene limitaciones visibles en el gráfico de distribución de longitud:
* El modelo conoce ~400,000 palabras.
* Si intentamos buscar una palabra muy moderna (ej. "COVID-19" o jerga de 2024) o mal escrita que no esté en ese vocabulario fijo, el modelo fallará (Error `KeyError`). A diferencia del Tokenizador BPE que vimos antes (que rompe las palabras desconocidas en partes), los modelos de embeddings estáticos como GloVe requieren que la palabra exacta exista en su diccionario.

### Conclusión
Los embeddings permiten a las máquinas entender analogías y relaciones. No entienden el concepto filosófico de una "manzana", pero entienden perfectamente que:
$$\text{Manzana} - \text{Fruta} + \text{Tecnología} \approx \text{Apple (la empresa)}$$
*(Esta es la famosa propiedad de las analogías vectoriales).*
"""

